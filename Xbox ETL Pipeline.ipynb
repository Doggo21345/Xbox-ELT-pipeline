{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ddaf1a8-b9cb-4e70-abb9-467b1fb45df9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"fs.azure.account.auth.type.gpelt.dfs.core.windows.net\", \"SAS\")\n",
    "spark.conf.set(\"fs.azure.sas.token.provider.type.gpelt.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.sas.FixedSASTokenProvider\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4239981c-0b38-46e7-a236-f7978c322dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def prepare_games_dataset_cloud(json_path):\n",
    "    print(f\"Reading full file from: {json_path}\")\n",
    "    \n",
    "    # 1. Read from Azure using Spark\n",
    "    raw_spark_df = spark.read.option(\"multiline\", \"true\").json(json_path)\n",
    "    \n",
    "    # 2. Convert the entire dataset into a list of dictionaries\n",
    "    games_list = [row.asDict(recursive=True) for row in raw_spark_df.collect()]\n",
    "    \n",
    "    # 3. USE JSON NORMALIZE (This replaces the loop)\n",
    "    # This automatically turns nested dicts into 'rating_alltime.RatingCount', etc.\n",
    "    df = pd.json_normalize(games_list)\n",
    "    \n",
    "    # 4. Cleanup column names to match your Master CSV/ML logic\n",
    "    # We rename the 'dot' names to 'underscore' names\n",
    "    column_mapping = {\n",
    "        'rating_7_days.RatingCount': 'rating_7_days_count',\n",
    "        'rating_7_days.AverageRating': 'rating_7_days_avg',\n",
    "        'rating_30_days.RatingCount': 'rating_30_days_count',\n",
    "        'rating_30_days.AverageRating': 'rating_30_days_avg',\n",
    "        'rating_alltime.RatingCount': 'rating_alltime_count',\n",
    "        'rating_alltime.AverageRating': 'rating_alltime_avg',\n",
    "        'esrb': 'ESRB',\n",
    "        'category': 'Genre'\n",
    "    }\n",
    "    df = df.rename(columns=column_mapping)\n",
    "\n",
    "    # 5. Extract Price (since 'prices' is a list, normalize won't flatten it automatically)\n",
    "    # We create a simple helper to get the first list_price > 0\n",
    "    def get_price(price_list):\n",
    "        if not isinstance(price_list, list): return 0\n",
    "        return next((p.get('list_price', 0) for p in price_list if p.get('list_price', 0) > 0), 0)\n",
    "    \n",
    "    df['current_price'] = df['prices'].apply(get_price)\n",
    "    \n",
    "    # 6. Filter for active games\n",
    "    df_active = df[(df['rating_alltime_count'] > 0) | \n",
    "                  (df['rating_7_days_count'] > 0) | \n",
    "                  (df['rating_30_days_count'] > 0)].copy()\n",
    "    \n",
    "    return df_active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ab67be3-9428-40e2-949b-cf793c3328f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "publisher_map = {\n",
    "    # 2K / Take-Two\n",
    "    '2k': '2k',\n",
    "    '2k games': '2k',\n",
    "    '2k publishing': '2k',\n",
    "    'visual concepts': '2k',\n",
    "    'private division': 'take-two',\n",
    "    'rockstar games': 'take-two',\n",
    "    'gearbox software': 'take-two',\n",
    "    'hangar 13': 'take-two',\n",
    "    \n",
    "    # Activision Blizzard\n",
    "    'activision': 'activision',\n",
    "    'activision publishing inc.': 'activision',\n",
    "    'blizzard entertainment': 'blizzard',\n",
    "    \n",
    "    # Bandai Namco / D3\n",
    "    'bandai namco entertainment america': 'bandai namco',\n",
    "    'bandai namco entertainment': 'bandai namco',\n",
    "    'bandai namco entertainment america inc.': 'bandai namco',\n",
    "    'bandai namco entertainment inc.': 'bandai namco',\n",
    "    'bandai namco studios inc.': 'bandai namco',\n",
    "    'bandai namco studios': 'bandai namco',\n",
    "    'namco bandai games america': 'bandai namco',\n",
    "    'namco bandai games america inc.': 'bandai namco',\n",
    "    'd3 publisher': 'd3 publisher',\n",
    "    'd3publisher of america, inc.': 'd3 publisher',\n",
    "    'd3publisher of america, inc': 'd3 publisher',\n",
    "    'd3publisher, inc.': 'd3 publisher',\n",
    "    'd3 publisher of america, inc.': 'd3 publisher',\n",
    "    'd3 publisher of america inc.': 'd3 publisher',\n",
    "    \n",
    "    # Capcom\n",
    "    'capcom': 'capcom',\n",
    "    'capcom u.s.a., inc': 'capcom',\n",
    "    'capcom entertainment, inc.': 'capcom',\n",
    "    '株式会社 カプコン': 'capcom',\n",
    "    'capcom co., ltd.': 'capcom',\n",
    "    'capcom co,. ltd.': 'capcom',\n",
    "    \n",
    "    # Daedalic / Nacon\n",
    "    'daedalic entertainment': 'daedalic',\n",
    "    'daedalic entertainment gmbh': 'daedalic',\n",
    "    'big ant studios': 'nacon',\n",
    "    'big ant studios pty': 'nacon',\n",
    "    'passtech games': 'nacon',\n",
    "    \n",
    "    # Deep Silver / Embracer\n",
    "    'deep silver': 'deep silver',\n",
    "    'deep silver dambuster studios': 'deep silver',\n",
    "    'deep silver, inxile entertainment': 'deep silver',\n",
    "    'deep silver, koch media': 'deep silver',\n",
    "    'koch media': 'deep silver',\n",
    "    'prime matter': 'deep silver',\n",
    "    'thq nordic': 'thq nordic',\n",
    "    'thq nordic gmbh': 'thq nordic',\n",
    "    'nordic games': 'thq nordic',\n",
    "    'nordic games gmbh': 'thq nordic',\n",
    "    'coffee stain publishing': 'coffee stain',\n",
    "    'coffee stain publishing ab': 'coffee stain',\n",
    "    'milestone s.r.l.': 'milestone',\n",
    "    'ravenscourt': 'ravenscourt',\n",
    "    'ravenscourt games': 'ravenscourt',\n",
    "    \n",
    "    # Disney\n",
    "    'disney': 'disney',\n",
    "    'disney interactive studios': 'disney',\n",
    "    'disney interactive': 'disney',\n",
    "    'disney bolt': 'disney',\n",
    "    'lucasfilm': 'disney',\n",
    "    'lucasarts': 'disney',\n",
    "    \n",
    "    # Electronic Arts\n",
    "    'electronic arts': 'electronic arts',\n",
    "    'electronic arts ': 'electronic arts',\n",
    "    'electronic arts inc.': 'electronic arts',\n",
    "    'ea': 'electronic arts',\n",
    "    'ea sports': 'electronic arts',\n",
    "    'ea sports™': 'electronic arts',\n",
    "    'ea vancouver': 'electronic arts',\n",
    "    'ea digital illusions ce ab': 'electronic arts',\n",
    "    'codemasters': 'electronic arts',\n",
    "    'codemasters software company limited': 'electronic arts',\n",
    "    'popcap': 'electronic arts',\n",
    "    'popcap games': 'electronic arts',\n",
    "    'popcap games, inc.': 'electronic arts',\n",
    "    \n",
    "    # Focus Entertainment\n",
    "    'focus entertainment': 'focus entertainment',\n",
    "    'focus home interactive': 'focus entertainment',\n",
    "    'dotemu': 'focus entertainment',\n",
    "    'the arcade crew': 'focus entertainment',\n",
    "    \n",
    "    # Microsoft / Xbox Game Studios\n",
    "    'xbox game studios': 'xbox game studios',\n",
    "    'xbox game studios ': 'xbox game studios',\n",
    "    'xbox games studios, rare ltd': 'xbox game studios',\n",
    "    'microsoft': 'xbox game studios',\n",
    "    'microsoft studios': 'xbox game studios',\n",
    "    'microsoft game studios': 'xbox game studios',\n",
    "    'microsoft corporation': 'xbox game studios',\n",
    "    'xbox live arcade': 'xbox game studios',\n",
    "    '343 industries': 'xbox game studios',\n",
    "    'the coalition': 'xbox game studios',\n",
    "    'turn 10 studios': 'xbox game studios',\n",
    "    'rare': 'xbox game studios',\n",
    "    'mojang studios': 'xbox game studios',\n",
    "    'double fine': 'xbox game studios',\n",
    "    'double fine productions': 'xbox game studios',\n",
    "    'double fine productions, inc.': 'xbox game studios',\n",
    "    \n",
    "    # SEGA\n",
    "    'sega': 'sega',\n",
    "    'sega europe ltd': 'sega',\n",
    "    'sega of america': 'sega',\n",
    "    'sega of america, inc.': 'sega',\n",
    "    'ryu ga gotoku studio': 'sega',\n",
    "    \n",
    "    # Square Enix\n",
    "    'square enix': 'square enix',\n",
    "    'square enix ltd': 'square enix',\n",
    "    'square enix co., ltd.': 'square enix',\n",
    "    'square enix ltdio-interactive a/s': 'square enix',\n",
    "    'square enix ltdi-interactive a/s': 'square enix',\n",
    "    \n",
    "    # Team17\n",
    "    'team17': 'team17',\n",
    "    'team17 ': 'team17',\n",
    "    'team 17': 'team17',\n",
    "    'team17 digital ltd': 'team17',\n",
    "    'team17 digital ltd.': 'team17',\n",
    "    \n",
    "    # tinyBuild\n",
    "    'tinybuild': 'tinybuild',\n",
    "    'tinybuild games': 'tinybuild',\n",
    "    \n",
    "    # Others\n",
    "    'another indie': 'another indie',\n",
    "    'another indie studio': 'another indie',\n",
    "    'aspyr': 'aspyr',\n",
    "    'aspyr media': 'aspyr',\n",
    "    'astragon entertainment': 'astragon entertainment',\n",
    "    'astragon entertainment gmbh': 'astragon entertainment',\n",
    "    'fatshark': 'fatshark',\n",
    "    'fatshark ab': 'fatshark',\n",
    "    'fatshark studios ab': 'fatshark',\n",
    "    'flashbulb': 'flashbulb',\n",
    "    'flashbulb games': 'flashbulb',\n",
    "    'headup': 'headup',\n",
    "    'headup games': 'headup',\n",
    "    'headup gmbh': 'headup',\n",
    "    'hypetrain digital': 'hypetrain digital',\n",
    "    'image & form': 'image & form',\n",
    "    'image & form games': 'image & form',\n",
    "    'image & form international ab': 'image & form',\n",
    "    'jackbox games': 'jackbox games',\n",
    "    'jackbox games, inc.': 'jackbox games',\n",
    "    'kalypso media': 'kalypso media',\n",
    "    'kalypso media group gmbh': 'kalypso media',\n",
    "    'kalypso media group': 'kalypso media',\n",
    "    'konami': 'konami',\n",
    "    'konami digital entertainment': 'konami',\n",
    "    'konami digital entertainment, inc.': 'konami',\n",
    "    'playway': 'playway',\n",
    "    'playway sa': 'playway',\n",
    "    'playway s.a.': 'playway',\n",
    "    'pm studios, inc.': 'pm studios',\n",
    "    'pm-studios, inc.': 'pm studios',\n",
    "    'spike': 'spike chunsoft',\n",
    "    'spike chunsoft': 'spike chunsoft',\n",
    "    'spike chunsoft co., ltd.': 'spike chunsoft',\n",
    "    'telltale': 'telltale',\n",
    "    'telltale games': 'telltale',\n",
    "    'valve': 'valve',\n",
    "    'versus evil': 'versus evil',\n",
    "    'versus evil, llc.': 'versus evil',\n",
    "    'arc system works': 'arc systems',\n",
    "    'ubisoft entertainment': 'ubisoft',\n",
    "    'wired productions ltd': 'wired productions',\n",
    "    'snk playmore corporation':'snk playmore',\n",
    "    'unknown worlds entertainment': 'unknown worlds',\n",
    "    'thq nordic': 'thq',\n",
    "    'tecmo koei america': 'tecmo'\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5613a2ce-6593-427b-9e29-789dfbd0ae84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def calculate_game_metrics(df):\n",
    "    \"\"\"Add calculated metric columns directly to DataFrame.\"\"\"\n",
    "    # Get rating counts\n",
    "    r7 = pd.to_numeric(df[\"rating_7_days_count\"], errors='coerce').fillna(0)\n",
    "    r30 = pd.to_numeric(df[\"rating_30_days_count\"], errors='coerce').fillna(0)\n",
    "    r_all = pd.to_numeric(df[\"rating_alltime_count\"], errors='coerce').fillna(0)\n",
    "    \n",
    "    # Get ratings\n",
    "    rating_7d = pd.to_numeric(df[\"rating_7_days_avg\"], errors='coerce').fillna(0)\n",
    "    rating_30d = pd.to_numeric(df[\"rating_30_days_avg\"], errors='coerce').fillna(0)\n",
    "    rating_all = pd.to_numeric(df[\"rating_alltime_avg\"], errors='coerce').fillna(0)\n",
    "\n",
    "    \n",
    "    # Parse dates\n",
    "    release_date = pd.to_datetime(df[\"Release\"], errors='coerce')\n",
    "    gamepass_date = pd.to_datetime(df[\"Added\"], errors='coerce')\n",
    "\n",
    "    # Normalize timezones: make sure datetimes are tz-naive so subtraction works\n",
    "    # If series are timezone-aware, convert to naive by removing tz info.\n",
    "    try:\n",
    "        if getattr(release_date.dt, 'tz', None) is not None:\n",
    "            release_date = release_date.dt.tz_convert(None)\n",
    "    except Exception:\n",
    "        # fallback: attempt elementwise removal\n",
    "        release_date = release_date.apply(lambda x: x.tz_convert(None) if getattr(x, 'tzinfo', None) is not None else x)\n",
    "\n",
    "    try:\n",
    "        if getattr(gamepass_date.dt, 'tz', None) is not None:\n",
    "            gamepass_date = gamepass_date.dt.tz_convert(None)\n",
    "    except Exception:\n",
    "        gamepass_date = gamepass_date.apply(lambda x: x.tz_convert(None) if getattr(x, 'tzinfo', None) is not None else x)\n",
    "    \n",
    "    # Calculate time deltas using a tz-naive 'now'\n",
    "    now = pd.Timestamp.now()\n",
    "    df['days_since_release'] = (now - release_date).dt.days\n",
    "    df['days_since_gp_add'] = (now - gamepass_date).dt.days\n",
    "    \n",
    "    # Calculate metrics\n",
    "    df['momentum'] = ((r7 / r30 * 100).fillna(0)).round(2)\n",
    "    df['discovery_capture'] = ((r7 / r_all * 100).fillna(0)).round(2)\n",
    "    df['quality_retention'] = (rating_30d - rating_all).round(3)\n",
    "    df['rating_trend_7d_vs_alltime'] = (rating_7d - rating_all).round(3)\n",
    "    df['is_day_one_gp'] = (df['days_since_gp_add'] <= 1) & (gamepass_date.notna())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ee73c06-f569-4bfd-8de3-b1b4f794401c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{\"days_since_release\":248},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"MS_Store_Link\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1767829372391}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[],\"syncTimestamp\":1768011668053}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Load your Master CSV (the one with the schema you provided)\n",
    "# If stored in DBFS or Azure, adjust the path\n",
    "spark_master_df = spark.table(\"hive_metastore.default.xbox_final_cleaned_results\")\n",
    "\n",
    "# 2. Convert it to Pandas so the rest of your script works\n",
    "master_df = spark_master_df.toPandas()\n",
    "\n",
    "# 2. Get the latest JSON telemetry from the Azure scrape\n",
    "folder_path = \"abfss://xbox-data@gpelt.dfs.core.windows.net/scrapes/\"\n",
    "files = dbutils.fs.ls(folder_path)\n",
    "latest_json_path = max(files, key=lambda f: f.modificationTime).path\n",
    "\n",
    "# 3. Pull telemetry using your existing function\n",
    "# This gets the rating_counts and current_price\n",
    "df_telemetry = prepare_games_dataset_cloud(latest_json_path)\n",
    "\n",
    "# 4. THE MASTER MERGE (On ProductID)\n",
    "# We left-join telemetry onto the Master CSV\n",
    "final_df = pd.merge(\n",
    "    master_df,\n",
    "    df_telemetry,\n",
    "    left_on='ProductID',\n",
    "    right_on='product_id',\n",
    "    how='inner' # Use 'inner' to only keep games that were actually scraped successfully\n",
    ")\n",
    "\n",
    "# 5. RUN FINAL ENRICHMENT\n",
    "final_df = calculate_game_metrics(final_df)\n",
    "final_df['publisher'] = final_df['publisher'].str.strip().str.lower().replace(publisher_map)\n",
    "\n",
    "# 6. DROP DUPLICATE/CLEANUP COLUMNS\n",
    "# Since we have both 'product_id' and 'ProductID', drop one\n",
    "final_df = final_df.drop_duplicates(subset=['ProductID'])\n",
    "\n",
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15f5764a-aa33-4561-a048-f4932633b4d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "final_df['is_day_one_gp'] = np.where(final_df['days_since_gp_add'] == final_df['days_since_release'], True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a8940c0-94cd-4184-b678-2328f5c874a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def backfill_columns(DF):\n",
    "    # 1. Fix the mixed-type columns FIRST\n",
    "    # Convert lists to strings, but keep existing strings as they are\n",
    "    mixed_cols = ['ESRB Content Descriptors', 'esrb_descriptors']\n",
    "    \n",
    "    for col in mixed_cols:\n",
    "        if col in DF.columns:\n",
    "            DF[col] = DF[col].apply(lambda x: \", \".join(x) if isinstance(x, list) else x)\n",
    "\n",
    "    # 2. Proceed with your backfill logic\n",
    "    DF['Genre'] = DF['Genre_x'].fillna(DF['Genre_y'])\n",
    "    DF['ESRB_x'] = DF['ESRB_x'].fillna(DF['ESRB_y'])\n",
    "    DF['ESRB Content Descriptors'] = DF['ESRB Content Descriptors'].fillna(DF['esrb_descriptors'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Note: Ensure 'Series X|S' is correctly mapped to 'is_xpa' if that was your intent\n",
    "    DF['Series X|S'] = DF['Series X|S'].fillna(\"No relation\")\n",
    "    DF['xCloud'] = DF ['xCloud'].fillna(\"None\")\n",
    "    return DF\n",
    "\n",
    "# Apply the fix to your Pandas DataFrame\n",
    "DF = backfill_columns(final_df)\n",
    "\n",
    "# Now the conversion to Spark will work without the merge error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14646f29-e12d-45d7-a7ae-60c71d7f77e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def drop_columns(df): \n",
    "    columns_to_drop = ['Months', 'Completion', 'Age', 'Status.1', 'Added', 'Owner Notes', 'Genre_y', 'title', 'bundle_count', 'short_description', 'esrb_descriptors', 'product_id', 'Added.1', 'Delay', 'ESRB_y', 'prices', 'Community Notes', 'is_xpa']\n",
    "    df = df.drop(columns=columns_to_drop)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "555c4b31-dc0e-4477-a437-6fac3bb75783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = drop_columns(final_df)\n",
    "def rename_columns(df):\n",
    "    df = df.rename(columns={'Game': 'title', 'MS_Store_Link': 'url'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ced2448-c16d-4e87-a3a9-e5da9e2ae468",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def add_comprehensive_baselines(df):\n",
    "    # Metrics we want to analyze\n",
    "    metrics = ['momentum', 'discovery_capture', 'quality_retention']\n",
    "    \n",
    "    for metric in metrics:\n",
    "        # 1. Calculate the 'Genre' Median (Broad Baseline)\n",
    "        df[f'genre_median_{metric}'] = df.groupby('Genre')[metric].transform('median')\n",
    "        df[f'lift_vs_genre_{metric}'] = df[metric] - df[f'genre_median_{metric}']\n",
    "\n",
    "        \n",
    "        # 2. Calculate the 'Genre + ESRB' Median (Conditioned Baseline)\n",
    "        # This gives you the context: \"How do M-rated Action games usually perform?\"\n",
    "        df[f'conditioned_median_{metric}'] = df.groupby(['Genre', 'ESRB_x'])[metric].transform('median')\n",
    "        \n",
    "        # 3. Calculate the 'Relative Lift' (for Streamlit charts)\n",
    "        # Positive = Overperforming, Negative = Underperforming\n",
    "        df[f'lift_vs_rating_{metric}'] = df[metric] - df[f'conditioned_median_{metric}']\n",
    "\n",
    "        \n",
    "        # 4. Standardized ML Feature (Robust Z-Score)\n",
    "        # For ML, we use Median Absolute Deviation (MAD) if data is skewed, \n",
    "        # but a standard STD works for basic normalization\n",
    "        std_dev = df.groupby(['Genre', 'ESRB_x'])[metric].transform('std')\n",
    "        df[f'zscore_{metric}'] = (df[metric] - df[f'conditioned_median_{metric}']) / std_dev\n",
    "        \n",
    "    return df\n",
    "\n",
    "# Apply to your main DataFrame\n",
    "DF = add_comprehensive_baselines(final_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "507c7f6f-afcb-47fd-bc76-1b2c4fb62cdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "final_df = final_df.drop_duplicates(subset=['ProductID'])\n",
    "final_df = final_df.rename(columns={'Series X_S': 'Series X|S', 'MS_Store_Link': 'url'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c245a406-3e9e-4285-8064-7ea2fe13f243",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"url\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1768162285945}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cd7053d-db50-48b0-b653-34e01ac098b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1. Convert Pandas DF to Spark DF\n",
    "# This is where the 'CANNOT_MERGE_TYPE' error usually happens\n",
    "final_df.columns = [col.replace('|', '_').replace(' ', '_').replace(';', '').replace('{', '').replace('}', '').replace('(', '').replace(')', '').replace('\\n', '').replace('\\t', '').replace('=', '') for col in final_df.columns]; spark_df = spark.createDataFrame(final_df) \n",
    "\n",
    "# 2. Save as a Delta Table\n",
    "# This makes it queryable via SQL: SELECT * FROM xbox_analysis_data\n",
    "spark_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"xbox_analysis_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "02b24101-f18d-4495-9691-99a88a58c9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE DETAIL hive_metastore.default.xbox_analysis_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcd4c6b0-58d6-414b-8775-f1ebdd303d87",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def Genre_performance_analysis(df):\n",
    "    \"\"\"Analyze performance metrics by Genre.\"\"\"\n",
    "    Genre_stats = df.groupby('Genre').agg({\n",
    "        'momentum': ['median', 'mean', 'std'],\n",
    "        'discovery_capture': ['median', 'mean'],\n",
    "        'quality_retention': ['median', 'mean'],\n",
    "        'rating_7_days_count': ['mean', 'std', 'median'],\n",
    "        'rating_30_days_count': ['mean', 'std', 'median'],\n",
    "        'rating_alltime_count': ['mean', 'std', 'median'],\n",
    "        'rating_alltime_avg': ['mean', 'std', 'median'],\n",
    "        'rating_30_days_avg': ['mean', 'std', 'median'],\n",
    "        'rating_7_days_avg': ['mean', 'std', 'median'],\n",
    "        'rating_trend_7d_vs_alltime': ['mean', 'std', 'median'],\n",
    "        'Game': 'count'  # Number of games per Genre\n",
    "    }).round(2)\n",
    "    \n",
    "    \n",
    "    Genre_stats.columns = ['_'.join(col).strip() for col in Genre_stats.columns.values]\n",
    "    Genre_stats = Genre_stats.reset_index()\n",
    "    \n",
    "    return Genre_stats\n",
    "\n",
    "def Genre_gamepass_comparison(df):\n",
    "    \"\"\"Compare Game Pass vs Non-Game Pass games by Genre.\"\"\"\n",
    "    comparison = df.groupby(['Genre', 'has_gamepass_remediation']).agg({ #using the agg fucntion to peform a series of operations on the grouped data to get summary statistics for each Genre and Game Pass status\n",
    "        'momentum': ['mean', 'std', 'median'],\n",
    "        'discovery_capture': ['mean', 'std', 'median'],\n",
    "        'quality_retention': ['mean', 'std', 'median'],\n",
    "        'rating_7_days_avg': ['mean', 'std', 'median'],\n",
    "        'rating_30_days_avg': ['mean', 'std', 'median'],\n",
    "        'rating_alltime_avg': ['mean', 'std', 'median'],\n",
    "        'rating_7_days_count': ['mean', 'std', 'median'],\n",
    "        'rating_30_days_count': ['mean', 'std', 'median'],\n",
    "        'rating_alltime_count': ['mean', 'std', 'median'],\n",
    "        'has_gamepass_remediation': 'sum',  # Number of games on GP\n",
    "        'Game': 'count'  # Total games\n",
    "    }).round(2)\n",
    "    # 2. Use the same spelling consistently\n",
    "    comparison = comparison.rename(columns={'title': 'game_count'}) \n",
    "    \n",
    "    # Fixed the typo from 'comparsion' to 'comparison' below\n",
    "    comparison.columns = ['_'.join(col).strip() for col in comparison.columns.values]\n",
    "    comparison = comparison.reset_index()\n",
    "    \n",
    "    return comparison\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9560b089-22eb-4b23-91ca-4676ebbe1e2a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1768162393551}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Genre_perf = Genre_performance_analysis(final_df)\n",
    "display(Genre_perf)\n",
    "spark.createDataFrame(Genre_perf).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_xbox_genre_performance\")\n",
    "\n",
    "# 2. Generate the Game Pass Comparison\n",
    "Genre_gp = Genre_gamepass_comparison(final_df)\n",
    "display(Genre_gp)\n",
    "spark.createDataFrame(Genre_gp).write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"gold_xbox_gamepass_impact\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639c2f8f-4807-4f97-b548-e717c593c1c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# rename base column and merge\n",
    "genre_all_baseline = Genre_perf[['Genre','momentum_mean', 'discovery_capture_median', 'quality_retention_median']].rename(columns={'momentum_mean':'momentum_genre_baseline', 'discovery_capture_median': 'discovery_capture_baseline', 'quality_retention_median' : 'quality_retention_baseline'})\n",
    "merged = Genre_gp.merge(genre_all_baseline, on='Genre', how='left')\n",
    "\n",
    "# differences (row-level: GP True/False rows will get baseline)\n",
    "merged['momentum_diff_vs_baseline'] = merged['momentum_mean'] - merged['momentum_genre_baseline']\n",
    "merged['momentum_pct_vs_baseline'] = merged['momentum_diff_vs_baseline'] / merged['momentum_genre_baseline'].replace(0,np.nan) * 100\n",
    "merged['discovery_capture_diff_vs_baseline'] = merged['discovery_capture_median'] - merged['discovery_capture_baseline']\n",
    "merged['quality_retention_vs_baseline'] = merged['quality_retention_median'] - merged['quality_retention_baseline']\n",
    "merged['quality_pct_vs_baseline'] = merged['quality_retention_vs_baseline'] / merged['quality_retention_baseline'].replace(0,np.nan) * 100\n",
    "\n",
    "\n",
    "\n",
    "merged.head(10)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5290521356639402,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Xbox ETL Pipeline",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
